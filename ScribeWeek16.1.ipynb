{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 16.1 Overfitting, Test and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn's train_test_split function allows you to easily split your dataset into a training and test set. Typically in machine learning it is common practice to split the data 60/20/20 for training, validation, and test set respectively. The code below downloads the MNIST dataset. It returns and training and a test set, we take the training set and split it 80/20 and can use the 20% has a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size\n",
      "(48000, 28, 28)\n",
      "(48000,)\n",
      "\n",
      "Validation data size\n",
      "(12000, 28, 28)\n",
      "(12000,)\n",
      "\n",
      "Testing data size\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# By splitting the training set we should have 48000 samples for training and 12000 for validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20)\n",
    "print(\"Training data size\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print(\"Validation data size\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print()\n",
    "print(\"Testing data size\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST is a dataset containing 60000 images of size (28, 28) of a number between 0 and 9, so we have 10 possible classes. If we want to make sure our training set contains enough examples for each class we can check the frequency of each class in our training data. Each class is represented as an integer from 0 to 9. Since we are working with numpy we can use np.bincount to count the number of times each class occurs. The array returned is the count of the class, where the class is the index of the array entry. By plotting it out we wee that the classes are equally distributed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = np.bincount(y_train.reshape(48000,))\n",
    "class_freqs = dict(enumerate(counts))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(class_freqs)), list(class_freqs.values()))\n",
    "plt.xticks(range(len(class_freqs)), list(class_freqs.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this was a much larger dataset it would have been better to draw samples from the dataset and then take a look at this sample. In the case of having unequal representation in the dataset we could utilize stratified sampling by breaking the dataset into groups, such as breaking the data into batches of size K and then uniformly sample n times from each batch (n would have to be smaller than K). This helps with observing any class that may be a rarity in the dataset and could also help with prototyping and testing since we would be working with a smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping samples with missing values is okay for datasets that are large, but if we have many samples with missing values or if the dataset is small it would be more ideal to try to predict those missing values from other features. Besides setting these missing values as the minimum, maximum, mean, or median of that feature column we could look at a correlation matrix of size N x N, where N is the number of features, and find which features are more correlated with the feature column containing the missing values. If we find features that have high correlation with the missing features than the missing features are redundant because the correlated columns then contain a lot of information about the behavior of the missing feature. If no correlations are found we can throw the non-missing features into a model and use them to predict the missing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling: min-max and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scaling features we need to make sure the features don't have different scales. One way to scale the features is to use min-max scaling which compresses the features to be in the interval $[0,1]$. Another method is standardize the features by z-score normalization. This converts to features to be described as steps above or below the mean of the specific feature and is not bounded like min-max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important ot apply the same type of scaling to both the train and test set. This is involves keeping the parameters used for scaling the training set to also scale the test set. One way of doing this is to use StandardScaler from sklearn. We reshape the samples from (28, 28) to (784,) because StandardScaler expects features to be 1-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "scaler = StandardScaler()                        # StandardScaler performs standardization on the data set\n",
    "scaler.fit_transform(x_train)                    # You can fit the transformation on the training set\n",
    "    \n",
    "scaler.transform(x_test)                         # And then use the same transformation parameters obtained from the \n",
    "                                                 # training set on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access the means and variance for each feature\n",
    "scaler.mean_\n",
    "scaler.var_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
